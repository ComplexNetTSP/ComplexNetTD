{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f11dc9",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/1/1d/Logo_T%C3%A9l%C3%A9com_SudParis.svg/1014px-Logo_T%C3%A9l%C3%A9com_SudParis.svg.png\" width=\"10%\" />\n",
    "</center>\n",
    "\n",
    "<center> <h2> NET 4103/7431 Complex Network </h2> </center>\n",
    "\n",
    "<center> <h3> Vincent Gauthier (vincent.gauthier@telecom-sudparis.eu) </h3> </center>\n",
    "\n",
    "### Note\n",
    "Avant de commencer les exercices, assurez-vous que tout fonctionne comme prévu. Tout d'abord, le redémarrage du kernel **(dans la barre de menus, sélectionnez le kernel $\\rightarrow$ Restart)**.\n",
    "\n",
    "Assurez-vous que vous remplir les célluler aux endroits marquer «YOUR CODE HERE». \n",
    "\n",
    "Veuillez supprimer les ligne «raise NotImplementedError()» dans toutes les cellules auxquelles vous avez répondu, ainsi que votre nom et prénom ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31237ad3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NOM = \"XXX\"\n",
    "PRENOM = \"XXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc47a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a540f",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lab #4: Graph Neural Networks</h1> \n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"../../images/network.png\" style=\"display:block;margin-left:auto;margin-right:auto;width:80%;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Style pour le Notebook\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452c8a5-35f3-477e-a249-dcf2f788f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch geometric\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
    "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'cuda version: {torch.version.cuda}')\n",
    "    TORCH = torch.__version__.split('+')[0]\n",
    "    CUDA = 'cpu' # or cu116 cu113\n",
    "\n",
    "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-geometric\n",
    "    import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccd265-d5b7-4e63-a38b-bd99cc8c3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from packaging import version\n",
    "import sys \n",
    "import sknetwork as skn\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"networkx version:\", nx.__version__)\n",
    "\n",
    "# assert networkx version is greater or equal to 2.6\n",
    "assert version.parse(nx.__version__) >= version.parse(\"2.6\")\n",
    "\n",
    "# assert torch version is greater or equal to '1.10.0'\n",
    "assert version.parse(nx.__version__) >= version.parse(\"1.10.0\")\n",
    "\n",
    "# assert torch_geometric version is greater or equal to '2.1.0'\n",
    "assert version.parse(nx.__version__) >= version.parse(\"2.1.0\")\n",
    "\n",
    "# assert python version is greater that 3.7\n",
    "assert sys.version_info[0] == 3\n",
    "assert sys.version_info[1] >= 7  \n",
    "\n",
    "# If working in colab mount the drive filesystem \n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Working in colab')\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"working locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bdd9c-250a-47b8-bc2c-0862f8348039",
   "metadata": {},
   "source": [
    "# 0 - Tools\n",
    "\n",
    "### Lauch Tensorboard\n",
    "\n",
    "more information about [tensorboard](https://www.tensorflow.org/tensorboard)\n",
    "\n",
    "```shell\n",
    "> tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "### Lauch MLFlow \n",
    "\n",
    "more information about [MLFlow](https://mlflow.org/)\n",
    "\n",
    "```shell\n",
    "> mlflow ui\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af090a-76b2-4d5f-a660-ac0ba498df55",
   "metadata": {},
   "source": [
    "# 1 - Classification with Graph Convolutional Network Layer (GCN)\n",
    "\n",
    "The GCN layer is mathematically defined as:\n",
    "\n",
    "$$ \\mathbf{h}^{(l+1)}_i = \\sigma \\left( \\sum_{j \\in \\mathcal{N}_i \\cup \\{ i \\} } \\frac{1}{\\sqrt{deg(i)} \\cdot \\sqrt{deg(j)}} \\left( \\mathbf{W} \\cdot \\mathbf{h}^{(l)}_j \\right) + \\mathbf{b} \\right)$$\n",
    "\n",
    "where neighboring node features are first transformed by a weight matrix , normalized by their degree, and finally summed up. Lastly, we apply the bias vector  to the aggregated output. This formula can be divided into the following steps:\n",
    "\n",
    "1. Add self-loops to the adjacency matrix.\n",
    "2. Linearly transform node feature matrix.\n",
    "3. Compute normalization coefficients.\n",
    "4. Normalize node features in .\n",
    "5. Sum up neighboring node features (\"add\" aggregation).\n",
    "6. Apply a final bias vector.\n",
    "\n",
    "Steps 1-3 are typically computed before message passing takes place. Steps 4-5 can be easily processed using the **MessagePassing** base class. The full layer implementation is shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfb108-839a-462a-afc2-fbb7d5ca44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import numpy as np\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        h = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, h.size(0), dtype=h.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        out = self.propagate(edge_index, x=h, norm=norm)\n",
    "\n",
    "        # Step 6: Apply a final bias vector.\n",
    "        out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c9e08-f563-46f3-b783-76eeb7ba43aa",
   "metadata": {},
   "source": [
    "**GCNConv** inherits from **MessagePassing** with \"add\" propagation. All the logic of the layer takes place in its **forward()** method. Here, we first add self-loops to our edge indices using the **torch_geometric.utils.add_self_loops()** function (step 1), as well as linearly transform node features by calling the **torch.nn.Linear** instance (step 2).\n",
    "\n",
    "The normalization coefficients are derived by the node degrees $deg(i)$ for each node $i$ which gets transformed to $1/\\left( \\sqrt{deg(i)} \\sqrt{deg(j)}\\right)$ for each edge $(j, i) \\in \\mathcal{E}$. The result is saved in the tensor **norm** of shape **[num_edge, ]** (step 3).\n",
    "\n",
    "We then call **propagate()**, which internally calls **message()**, **aggregate()** and **update()**. We pass the node embeddings $\\mathbf{h}$ and the normalization coefficients **norm** as additional arguments for message propagation.\n",
    "\n",
    "In the **message()** function, we need to normalize the neighboring node features **x_j** by **norm**. Here, **x_j** denotes a lifted tensor, which contains the source node features of each edge, i.e., the neighbors of each node. Node features can be automatically lifted by appending **_i** or **_j** to the variable name. In fact, any tensor can be converted this way, as long as they hold source or destination node features.\n",
    "\n",
    "That is all that it takes to create a simple message passing layer. You can use this layer as a building block for deep architectures. Initializing and calling it is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb921e1-78e7-4a4e-80e6-96ad72a6fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs - Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dp_rate = dp_rate_linear\n",
    "        self.conv1 = GCNConv(c_in, c_hidden)\n",
    "        self.conv2 = GCNConv(c_hidden, c_hidden)\n",
    "        self.lin = Linear(c_hidden, c_out)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx - Index of batch element for each node\n",
    "        \"\"\"\n",
    "        # first GCN layer \n",
    "        x = self.conv1(x, edge_index)\n",
    "        # activation function \n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dp_rate, training=self.training)\n",
    "        # second GCN layer \n",
    "        x = self.conv2(x, edge_index)\n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dp_rate, training=self.training)\n",
    "        # Head \n",
    "        x = self.lin(x)\n",
    "        # training – apply dropout if is True. Default: True\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c164ba3-eb0e-4742-9712-33a071f0fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root=\"./data\", name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')  # False\n",
    "print(f'Has self-loops: {data.has_self_loops()}')  # False\n",
    "print(f'Is undirected: {data.is_undirected()}')  # True\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "label_dict = {\n",
    "    0: \"Theory\",\n",
    "    1: \"Reinforcement_Learning\",\n",
    "    2: \"Genetic_Algorithms\",\n",
    "    3: \"Neural_Networks\",\n",
    "    4: \"Probabilistic_Methods\",\n",
    "    5: \"Case_Based\",\n",
    "    6: \"Rule_Learning\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b38660-dd9a-4377-9f22-5ee978dacea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2d460-5421-435e-bfe0-5e1c003a1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, model, data, device=\"cpu\", debug=False):\n",
    "    data.to(device)\n",
    "    # Set the model variable to train=True\n",
    "    model.train()\n",
    "    # Step 1: reset the gradient of the back propagation\n",
    "    optimizer.zero_grad()\n",
    "    # Step 2: Forward pass\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # Step 3: compute the loss on the train_node (the negative log likelihood loss).  \n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss_val = F.nll_loss(out[data.val_mask], data.y[data.val_mask])\n",
    "    # Step 4: execute one backpropagation \n",
    "    loss.backward()\n",
    "    # Step 5: update the learning weight according to the learning rate\n",
    "    optimizer.step()\n",
    "    return float(loss), float(loss_val)\n",
    "        \n",
    "@torch.no_grad()\n",
    "def test(model, data, device=\"cpu\"):\n",
    "    from torchmetrics import Accuracy\n",
    "    \n",
    "    data.to(device)\n",
    "    model.eval()\n",
    "    accuracy = Accuracy()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "    \n",
    "    return (\n",
    "        accuracy(pred[data.train_mask], data.y[data.train_mask]),\n",
    "        accuracy(pred[data.val_mask], data.y[data.val_mask]), \n",
    "        accuracy(pred[data.test_mask], data.y[data.test_mask])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29938b26-3979-44e8-a31a-46f37ffd923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run_GCN_model(dataset, device=\"cpu\", lr=0.001, n_epoch=100, emb_size=50, dp_rate_linear=0.5):\n",
    "    data = dataset[0]\n",
    "    data.to(device)\n",
    "\n",
    "    # run name\n",
    "    comment = f\" GCN N_epoch = {n_epoch}, lr = {lr}, emb_size = {emb_size}, dropout = {dp_rate_linear}\"\n",
    "    \n",
    "    # initialize the model \n",
    "    model = Net(dataset.num_features, emb_size, dataset.num_classes, dp_rate_linear=dp_rate_linear).to(device)\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize Tensor board\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "    print(f\"### run : {comment} ###\")\n",
    "    \n",
    "    # execute n_epoch \n",
    "    for epoch in range(n_epoch):\n",
    "        loss, loss_val = train(optimizer, model, data, debug=True)\n",
    "        acc_train, acc_val, acc_test = test(model, data)\n",
    "\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', loss_val, epoch)\n",
    "        writer.add_scalar('Accuracy/train', acc_train, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', acc_val, epoch)\n",
    "        writer.add_scalar('Accuracy/test', acc_test, epoch)\n",
    "        if epoch % 10 == 0: \n",
    "            print(f'epoch {epoch:04d}, ' + \n",
    "                  f'train loss: {loss:.4f}, ' + \n",
    "                  f'train accuracy: {acc_train:.4f}, ' + \n",
    "                  f'validation accuracy: {acc_val:.4f}, ' +\n",
    "                  f'test accuracy: {acc_test:.4f}' \n",
    "                 )\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193dbf7-897c-4984-86aa-4a4005880779",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_GCN_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16427fc-d1ec-4971-92de-af282c8b4990",
   "metadata": {},
   "source": [
    "# 2 - GraphSAGE\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1100/0*BpHK8HwjeCtSw0Uh\" alt=\"alt text\" title=\"image Title\" width=\"50%\">\n",
    "\n",
    "A first natural question is: Why shift from standard **GCN** to **GraphSAGE** ?\n",
    "\n",
    "**As a result, GCNs are not very practical, limited in terms of memory when handling large networks, and even not suitable for some cases.**\n",
    "\n",
    "**GraphSAGE** overcomes the previous challenges while relying on the same mathematical principles as GCNs. It provides a general inductive framework that is able to generate node embeddings for new nodes.\n",
    "\n",
    "Introduced by the paper: _Inductive Representation Learning on Large Graphs_ [6] in 2017, **GraphSAGE, which stands for Graph SAmpling and AggreGatE**, has made a significant contribution to the GNN research area.\n",
    "\n",
    "### So how does GraphSAGE work concretely?\n",
    "\n",
    "Rather than training individual embeddings for each node, the model learns a function that generates embeddings by sampling and aggregating the features of the local neighborhood of a node.\n",
    "\n",
    "$$\\mathbf{h}^{(l+1)}_{\\mathcal{N}(v)} = AGGREGATE \\left( \\{ \\mathbf{h}_u^{(l)}, \\forall u \\in \\mathcal{N}(v) \\} \\right) $$\n",
    "\n",
    "$$ \\mathbf{h}_u^{(l+1)} = \\sigma \\left( W \\cdot CONCAT \\left( \\mathbf{h}_v^{(l)} ,  \\mathbf{h}^{(l+1)}_{\\mathcal{N}(v)} \\right) \\right)$$\n",
    "\n",
    "At each iteration, the model follows two different steps:\n",
    "\n",
    "1. Sample: Instead of using the entire neighborhood of a given node, the model uniformly samples a fixed-size set of neighbors.\n",
    "2. Aggregate: Nodes aggregate information from their local neighbors as shown in the equation below. In the original paper [6], three aggregation functions are considered:\n",
    "\n",
    "    - Mean aggregator: It consists in taking the average of the vectors of the neighboring nodes. Simple and efficient, this approach has led to good performances in the experiments carried out in the research paper. It is the one that has been retained in the application below.\n",
    "\n",
    "     - LSTM aggregator: This aggregator has the potential to benefit from the greater expressive capabilities of the LTSMs architecture. To adapt it to graphs that have no natural order, the aggregator is applied to a random permutation of the node’s neighbors.\n",
    "     \n",
    "     - Pooling aggregator: It consists in feeding a fully connected neural network with the vector of each neighbor. After this transformation, a maximum pooling operation per element is applied to aggregate the information on all the neighbors. It has yielded very good results in the experiments with the paper.\n",
    "\n",
    "### Subgraph Sampling \n",
    "\n",
    "One main advantage of **GrapheSage** is to be able to use subgraph samle during the learing phase and not the whole graph. We will use the pytorch_geometric **[NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader)** function to achive that.\n",
    "\n",
    "**Question:** Plot the subgraph provided by the function **[NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader)** explain what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea36c9e-d74c-43ce-adb8-1667f787a356",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6f4633bab21615335ef917565e5d93",
     "grade": true,
     "grade_id": "cell-2ce26343febd4a13",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "import networkx as nx\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# We sample the graph\n",
    "# We sample 10 node from the origial graph with there subgraph (here the two hops neigborhood)  \n",
    "# We limit the number of possible node:\n",
    "#    - at 1 hop to be 10 \n",
    "#    - at 2 hop to be 5 per node in the 1 hop neighborhood.\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask, num_neighbors=[10, 5], shuffle=True, batch_size=10)\n",
    "\n",
    "# we extract the edge list\n",
    "batch = next(iter(train_loader))\n",
    "e = batch.edge_index.numpy().T\n",
    "\n",
    "# create the networkx graph\n",
    "G = nx.from_edgelist(e)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500f0aa-c4ee-4589-b1cd-d8b34348539a",
   "metadata": {},
   "source": [
    "### GraphSAGE model\n",
    "\n",
    "Read the following paper [1] and improve the model bellow with the inputs found in the paper.\n",
    "\n",
    "### Reference \n",
    "[1] [Design Space of Graph Neural Networks, J. You, R. Ying, J. Leskovec. NeurIPS (2020)](https://proceedings.neurips.cc/paper/2020/file/c5c3d4fe6b2cc463c7d7ecba17cc9de7-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516dad4-34c9-46fe-9372-b619c13844ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SAGENet(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out):\n",
    "        \n",
    "        super(SAGENet, self).__init__()\n",
    "        self.GNN1 = SAGEConv(c_in, c_hidden, normalize=False)\n",
    "        self.GNN2 = SAGEConv(c_hidden, c_hidden, normalize=False)\n",
    "        self.lin = Linear(c_hidden, c_hidden)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.GNN1.reset_parameters()\n",
    "        self.GNN2.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1\n",
    "        x = self.GNN1(x, edge_index)\n",
    "        h_1 = F.relu(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        h_1 = self.GNN2(h_1, edge_index)\n",
    "        h_2 = F.relu(h_1)\n",
    "        \n",
    "        # Head \n",
    "        h_out = self.lin(h_2)\n",
    "        # training – apply dropout if is True. Default: True\n",
    "        return F.log_softmax(h_out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e1b3a-553b-4a1d-b4af-50c91f522e28",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d789f515618f280a7f3719d483b242f2",
     "grade": true,
     "grade_id": "cell-ca02f6d65fa0dbb5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImproveSAGENet(torch.nn.Module):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff9a2a-9dff-47b6-9990-aa92f90524c5",
   "metadata": {},
   "source": [
    "### Training/Test/Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8764ed-5aea-4752-a05b-4fe13785a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, optimizer, model, device):\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        # Step 1: reset the gradient of the back propagation\n",
    "        optimizer.zero_grad()\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        # Step 2: Forward pass\n",
    "        y_hat = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "        # Step 3: compute the loss on the train_node (the negative log likelihood loss).  \n",
    "        loss = F.nll_loss(y_hat, y)\n",
    "        total_loss += float(loss) * batch.batch_size\n",
    "        total_examples += batch.batch_size\n",
    "        # Step 4: execute one backpropagation \n",
    "        loss.backward()\n",
    "        # Step 5: update the learning weight according to the learning rate\n",
    "        optimizer.step()\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, device=\"cpu\"):\n",
    "    from torchmetrics import Accuracy\n",
    "    \n",
    "    data.to(device)\n",
    "    model.eval()\n",
    "    accuracy = Accuracy()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "    \n",
    "    return (\n",
    "        accuracy(pred[data.train_mask], data.y[data.train_mask]),\n",
    "        accuracy(pred[data.val_mask], data.y[data.val_mask]), \n",
    "        accuracy(pred[data.test_mask], data.y[data.test_mask])\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(model, data, device=\"cpu\"):\n",
    "    data.to(device)\n",
    "    y = data.y\n",
    "    y_hat = model(data.x, data.edge_index)\n",
    "    return F.nll_loss(y_hat[data.val_mask], y[data.val_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a638dd-70d5-4902-9d16-2c354c71fb28",
   "metadata": {},
   "source": [
    "### Run the train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa878c-f148-4b50-97d0-bff70100e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run_SAGE_model(dataset, \n",
    "                   batch_size=32, \n",
    "                   lr=0.001, \n",
    "                   n_epoch=200, \n",
    "                   emb_size=50, \n",
    "                   dropout=0.5, \n",
    "                   device=\"cpu\", \n",
    "                   batchnorm=False): \n",
    "    \n",
    "    data = dataset[0]\n",
    "    \n",
    "    comment = f\" GrahSAGE n_epoch = {n_epoch}, lr = {lr}, emb_size = {emb_size}, dropout = {dropout}, batchnorm = {batchnorm}\"\n",
    "    \n",
    "    # Initialized the network\n",
    "    #model = SAGENet(dataset.num_features, emb_size, dataset.num_classes).to(device)\n",
    "    model = ImproveSAGENet(dataset.num_features, emb_size, dataset.num_classes, dropout=dropout, batchnorm=batchnorm).to(device)\n",
    "    \n",
    "    # Initialized the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Sample the graph\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask, num_neighbors=[10, 20], shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Initialize Tensor board\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "    print(f\"### run : {comment} ###\")\n",
    "    # execute n_epoch\n",
    "    best_acc_train, best_acc_val, best_acc_test, best_epoch = 0.0, 0.0, 0.0, 0\n",
    "    for epoch in range(1, n_epoch):\n",
    "        loss = train(train_loader, optimizer, model, device)\n",
    "        loss_val = validation(model, data)\n",
    "        acc_train, acc_val, acc_test = test(model, data)\n",
    "        \n",
    "        # Compute the best metric\n",
    "        if acc_val > best_acc_val:\n",
    "            best_acc_train = acc_train\n",
    "            best_acc_val = acc_val\n",
    "            best_acc_test = acc_test\n",
    "            best_model = model\n",
    "            best_epoch = epoch\n",
    "\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', loss_val, epoch)\n",
    "        writer.add_scalar('Accuracy/train', acc_train, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', acc_val, epoch)\n",
    "        writer.add_scalar('Accuracy/test', acc_test, epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch {epoch:04d}, train loss: {loss:.2f} ' +\n",
    "                  f'validation loss:{loss_val:.2f} ' + \n",
    "                  f'train accuracy: {acc_train:.2f}, ' + \n",
    "                  f'validation accuracy: {acc_val:.2f}, ' +\n",
    "                  f'test accuracy: {acc_test:.2f}' )\n",
    "            \n",
    "    return best_acc_train, best_acc_val, best_acc_test, best_epoch, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413bc60-6d78-4fd8-a602-5d923bdd302a",
   "metadata": {},
   "source": [
    "### Run the model and store the information with MLFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adcde6-7093-4929-af4b-d1ef62f77837",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b17c92b2aa5be2276ace11ebc2adaacf",
     "grade": true,
     "grade_id": "cell-f36379896249e229",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "import mlflow \n",
    "\n",
    "# Hyperparameters\n",
    "emb_size = 16 \n",
    "n_epoch = 50\n",
    "lr = 0.1\n",
    "batchnorm = False\n",
    "batch_size = 8\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Set an experiment name, which must be unique and case-sensitive.\n",
    "experiment = mlflow.set_experiment(\"GraphSage Classification\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    log_param(\"emb_size\", emb_size)\n",
    "    log_param(\"dropout\", dropout)\n",
    "    log_param(\"lr\", lr)\n",
    "    log_param(\"n_epoch\", n_epoch)\n",
    "    log_param(\"batchnorm\", batchnorm)\n",
    "    log_param(\"batch_size\", batch_size)\n",
    "    \n",
    "    acc_train, acc_val, acc_test, epoch, model = run_SAGE_model(dataset, \n",
    "                                                                batch_size=batch_size, \n",
    "                                                                lr=lr, \n",
    "                                                                n_epoch=n_epoch, \n",
    "                                                                emb_size=emb_size, \n",
    "                                                                dropout=dropout, \n",
    "                                                                batchnorm=batchnorm)\n",
    "    log_metric(\"acc_train\", acc_train)\n",
    "    log_metric(\"acc_val\", acc_val)\n",
    "    log_metric(\"acc_test\", acc_test)\n",
    "    log_metric(\"best epoch\", epoch)\n",
    "    mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a5a13-affd-45d0-911a-0a24722c94f5",
   "metadata": {},
   "source": [
    "# 3 - Edge-level tasks: Link prediction\n",
    "\n",
    "In some applications, we might have to predict on an edge-level instead of node-level.\n",
    "The most common edge-level task in GNN is link prediction.\n",
    "Link prediction means that given a graph, we want to predict whether there will be/should be an edge between two nodes or not.\n",
    "For example, in a social network, this is used by Facebook and co to propose new friends to you.\n",
    "Again, graph level information can be crucial to perform this task.\n",
    "The output prediction is usually done by performing a similarity metric on the pair of node features,\n",
    "which should be 1 if there should be a link, and otherwise close to 0.\n",
    "To keep the tutorial short, we will not implement this task ourselves.\n",
    "\n",
    "### decoder or link predictor \n",
    "\n",
    "for two node $n_1, n_2$ with their respective embedding $h_1, h_2$ if their $ \\sigma (h_1 \\cdot h_2) > 0.5$ (where $\\sigma$ is the sigmoid function) we predict that they should be a link between them.\n",
    "\n",
    "### Training\n",
    "For trainning we are using positive and negative sample in order to train our model. In order to learn useful, predictive representations in a fully unsupervised setting, we apply graph-based loss function to the output representations, $z_u, \\forall u \\in V$. Thegraph-based loss function encourages nearby nodes to have similar representations, while enforcing\n",
    "that the representations of disparate nodes are highly distinct\n",
    "\n",
    "$$ J_G(\\mathbf{z}_u) = - \\log( \\sigma(\\mathbf{z}_u^T \\mathbf{z}_v) ) - Q \\cdot \\mathbb{E}_{v_n \\sim P_n(v)} \\log(-\\mathbf{z}_u^T \\mathbf{z}_n) $$\n",
    "\n",
    "\n",
    "where $v$ is a node that co-occurs near $u$, $\\sigma$ is the sigmoid function, $P_n$ is a negative sampling distribution, and $Q$ defines the number of negative sample.\n",
    "\n",
    "### Question \n",
    "\n",
    "* What is a good graction of negative sample ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218807b-87c6-4da5-b321-e6fc6e0b963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SAGELinkPreditor(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5):\n",
    "        \n",
    "        super(SAGELinkPreditor, self).__init__()\n",
    "\n",
    "        self.sage1 = SAGEConv(c_in, c_hidden, normalize=False)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(c_hidden)\n",
    "        self.sage2 = SAGEConv(c_hidden, c_hidden, normalize=False)\n",
    "        self.dropout = dp_rate_linear\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.sage1.reset_parameters()\n",
    "        self.sage2.reset_parameters()\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.sage1(x, edge_index)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.sage2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z, edge_label_index):\n",
    "        # cosine similarity\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b1008-ad76-4d68-aa3d-67119b392c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "def train(train_loader, model, optimizer, criterion, device=\"cpu\", frac_neg=1):\n",
    "    \"\"\"\n",
    "    Single epoch model training in batches.\n",
    "    :return: total loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.batch_size\n",
    "        \n",
    "        # generate embeddings\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        \n",
    "        # generate fake edges \n",
    "        neg_edge_index = negative_sampling(edge_index = batch.edge_index, \n",
    "                                           num_nodes = batch.num_nodes, \n",
    "                                           num_neg_samples = int(frac_neg*batch.edge_index.size(1)), \n",
    "                                           method = 'sparse')\n",
    "\n",
    "        # concatenate the real and the fake edges\n",
    "        edge_label_index = torch.cat([batch.edge_index, neg_edge_index], dim = -1)\n",
    "        \n",
    "        # create the grounth true label\n",
    "        edge_label = torch.cat([torch.ones(batch.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))], dim = 0)\n",
    "\n",
    "        # compute the cosine similarity between two node embeddings \n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(out, edge_label)\n",
    "\n",
    "        # standard torch mechanics here\n",
    "        # backpropagation \n",
    "        loss.backward()\n",
    "        # gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_examples += batch_size\n",
    "        total_loss += float(loss) * batch_size\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf9997-3815-48c5-a3a5-ec18e1d59452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, model, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "        Evalutes the model on the test set.\n",
    "    :param loader: the batch loader\n",
    "    :return: a score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    mean_auc = []\n",
    "    mean_acc = []\n",
    "    threshold = torch.tensor([0.7])\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        # Positive sample\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "       \n",
    "        \n",
    "        # negative sample\n",
    "        neg_edge_index = negative_sampling(edge_index = batch.edge_index, \n",
    "                                           num_nodes = batch.num_nodes, \n",
    "                                           num_neg_samples = None, \n",
    "                                           method = 'sparse')\n",
    "        \n",
    "        \n",
    "        # concatenate the real and the fake edges\n",
    "        edge_label_index = torch.cat([batch.edge_index, neg_edge_index], dim = -1)\n",
    "        \n",
    "        # create the grounth true label\n",
    "        edge_label = torch.cat([torch.ones(batch.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))], dim = 0)\n",
    "        \n",
    "        \n",
    "        out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "        pred = (out > threshold).float() * 1\n",
    "        \n",
    "        # score\n",
    "        score = f1_score(edge_label, pred.cpu().numpy())\n",
    "        auc = roc_auc_score(edge_label, pred.cpu().numpy(), average=None)\n",
    "        acc = accuracy_score(edge_label, pred.cpu().numpy())\n",
    "        \n",
    "        mean_auc.append(auc)\n",
    "        scores.append(score)\n",
    "        mean_acc.append(acc)\n",
    "    return np.average(scores), np.average(mean_auc), np.average(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14efc60-42bb-4acc-8676-dabc63350af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from datetime import datetime\n",
    "\n",
    "def run_link_prediction(data, emb_size = 50, dropout = 0.5, n_epoch = 100, lr = 0.001, frac_neg=1):\n",
    "    # run name\n",
    "    run_id = int(datetime.timestamp(datetime.now()))\n",
    "    comment = f\" Link prediction run_id={run_id} n_epoch = {n_epoch}, lr = {lr}, emb_size = {emb_size}, dropout = {dropout}\"\n",
    "\n",
    "    # Tensor board \n",
    "    writer = SummaryWriter(comment=comment)\n",
    "    \n",
    "    # Sample the graph\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask, num_neighbors=[20, 30], shuffle=True, batch_size=10)\n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask, num_neighbors=[20, 30], shuffle=True, batch_size=10)\n",
    "    val_loader = NeighborLoader(data, input_nodes=data.val_mask, num_neighbors=[20, 30], shuffle=True, batch_size=10)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SAGELinkPreditor(dataset.num_features, emb_size, emb_size, dropout)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=lr)\n",
    "\n",
    "    # BCELoss creates a criterion that measures the Binary Cross Entropy between the target and the output.\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"*** start the training: {comment} ***\")\n",
    "    best_test_f1, best_val_f1, test_auc, best_test_acc, best_model = 0.0, 0.0,0.0, 0.0, \"\"\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = train(train_loader, model, optimizer, criterion, frac_neg=frac_neg)\n",
    "        val_f1, val_auc, val_acc = test(val_loader, model)\n",
    "        test_f1, test_auc, test_acc = test(test_loader, model)\n",
    "        # compute the best metric\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_test_f1 = test_f1\n",
    "            best_test_auc = test_auc\n",
    "            best_test_acc = test_acc\n",
    "            best_model = model\n",
    "        print(f\"Epoch {epoch:03d} training loss {loss:.2f}, validation F1 Score: {val_f1}, test F1 Score: {test_f1}\")\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('F1Score/validation', val_f1, epoch)\n",
    "        writer.add_scalar('F1Score/test', test_f1, epoch)\n",
    "        writer.add_scalar('AUC/test', test_auc, epoch)\n",
    "        writer.add_scalar('AUC/validation', val_auc, epoch)\n",
    "        writer.add_scalar('Accuracy/test', val_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', test_acc, epoch)\n",
    "       \n",
    "    writer.close()\n",
    "    return best_val_f1, best_test_f1, best_test_auc, best_test_acc, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d804e-14fd-417f-9edf-cb81a0d5b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "import mlflow \n",
    "\n",
    "# Hyperparameters\n",
    "emb_size = 50 \n",
    "dropout = 0.5\n",
    "n_epoch = 100\n",
    "lr = 0.001\n",
    "frac_neg=5\n",
    "\n",
    "# Set an experiment name, which must be unique and case-sensitive.\n",
    "experiment = mlflow.set_experiment(\"GraphSage Link Prediction\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    log_param(\"emb_size\", emb_size)\n",
    "    log_param(\"dropout\", dropout)\n",
    "    log_param(\"lr\", lr)\n",
    "    log_param(\"n_epoch\", n_epoch)\n",
    "    log_param(\"frac_neg\", frac_neg)\n",
    "    val_f1, test_f1, test_auc, test_acc, model = run_link_prediction(data, emb_size=emb_size, dropout=dropout, n_epoch=n_epoch, lr=lr, frac_neg=frac_neg)\n",
    "    log_metric(\"val_f1\", val_f1)\n",
    "    log_metric(\"test_f1\", test_f1)\n",
    "    log_metric(\"test_acc\", test_acc)\n",
    "    log_metric(\"test_auc\", test_auc)\n",
    "    mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed60414-02b5-46e9-be15-982d6d92a45d",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "2. [Create a message passing networks with pytorch geometric](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html)\n",
    "3. [PyTorch Geometric example](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/link_pred.py)\n",
    "4. [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf), Zhou et al.\n",
    "2019\n",
    "5. [Link Prediction Based on Graph Neural Networks](https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf), Zhang and Chen, 2018.\n",
    "6.  [Inductive Representation Learning on Large Graphs](https://doi.org/10.48550/ARXIV.1706.02216),  Hamilton, W. L., Ying, R., & Leskovec, J., 2017.\n",
    "7. [A Comprehensive Case-Study of GraphSage with Hands-on-Experience using PyTorch-Geometric Library and Open-Graph-Benchmark’s Amazon Product Recommendation Dataset](https://towardsdatascience.com/a-comprehensive-case-study-of-graphsage-algorithm-with-hands-on-experience-using-pytorchgeometric-6fc631ab1067)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
